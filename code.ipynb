{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Variance Reduction Methods for Policy Evaluation\n",
    "\n",
    "## Convex Optimization: Project\n",
    "\n",
    "### Team Members:\n",
    "\n",
    "- Gautham Bellamkonda\n",
    "- Pranav Khalsanka Nayak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Consider the minimization of objective in equation (7) in https://proceedings.mlr.press/v70/du17a/du17a.pdf. Find a dual of min (7) by eliminating $\\theta$ from equation (10). Highlight the formula for computing the primal solution using the dual solution. Since the primal and dual are unconstrained quadratics, one can solve them using accelerated gradient descent. Note that the per iteration cost is $O(nd^2)$ (assume $n\\ge d$) for both primal and dual algos. We already know for reaching $\\epsilon$ optimal solution we need $\\log(\\frac{1}{\\epsilon})$ iterations. The paper proposes to solve something \"in-between\" the primal and dual (saddle-point form), which is equation (10). This problem has both primal and dual variables. The idea is to perform block coordinate (gradient based) descent\\&ascent  on primal\\&dual variables. The algorithm is in equation (12). The advantage is computing (12) requires only $O(nd)$. Interestingly, in theorem~1, they show this algorithm  also has linear convergence i.e., for reaching $\\epsilon$ optimal solution we need $\\log(\\frac{1}{\\epsilon})$ iterations. Thus primal and dual algorithms shall be worse than the proposed primal-dual algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a\n",
    "\n",
    "Derive a dual of min (7) by eliminating $\\theta$ from equation (10). Highlight the formula for computing the primal solution using the dual solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation (7) describes the minimization objective:\n",
    "\n",
    "$$\n",
    "\\text{MSPBE}(\\theta) = \\frac{1}{2} \\| \\hat A \\theta - \\hat b \\|^2_{\\hat C^{-1}} + \\frac{\\rho}{2} \\| \\theta \\|^2\n",
    "$$\n",
    "\n",
    "To minimize this objective with respect to $\\theta$, we introduce the dual variable $w$ and write the Lagrangian as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, w) = \\frac{\\rho}{2} \\| \\theta \\|^2 - w^T \\hat A \\theta - (\\frac{1}{2} w^T \\hat C w - w^T \\hat b)\n",
    "$$\n",
    "\n",
    "Minimizing MSPBE with respect to $\\theta$ is equivalent to solving \n",
    "\n",
    "$$\n",
    "\\min_{\\theta} \\max_{w} \\mathcal{L}(\\theta, w)\n",
    "$$\n",
    "\n",
    "Taking the gradient of $\\mathcal{L}(\\theta, w)$ with respect to $\\theta$ and setting it to zero, we get:\n",
    "\n",
    "$$\n",
    "\\rho \\theta - \\hat A^T w = 0\\\\\n",
    "\\theta = \\frac{\\hat A^T w}{\\rho}\n",
    "$$\n",
    "\n",
    "Substituting this back into the Lagrangian, we get the dual objective:\n",
    "\n",
    "$$\n",
    "\\max_{w} g(w) = \\max_{w} -\\frac{1}{2 \\rho} w^T \\hat A \\hat A^T w - (\\frac{1}{2} w^T \\hat C w - w^T \\hat b)\\\\\n",
    "= \\max_{w} -\\frac{1}{2 \\rho} w^T \\hat A \\hat A^T w - \\frac{1}{2} w^T \\hat C w + w^T \\hat b\\\\\n",
    "= \\max_{w} -\\frac{1}{2 \\rho} w^T (\\hat A \\hat A^T + \\rho \\hat C) w + w^T \\hat b\\\\\n",
    "= \\min_{w} \\frac{1}{2 \\rho} w^T (\\hat A \\hat A^T + \\rho \\hat C) w - w^T \\hat b\n",
    "$$\n",
    "\n",
    "The primal solution can be computed using the dual solution as:\n",
    "\n",
    "$$\n",
    "\\theta = \\frac{\\hat A^T w}{\\rho}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
